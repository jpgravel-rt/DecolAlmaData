---
title: "Alma Pulse Extraction Notebook"
output: html_notebook
resource_files:
- extract.sh
- data/decol-tags-recorded-2m.txt
- data/decol-tags-interpolatd-1s.txt
- data/decol-tags-avg-1h.txt
---

```{R}
defaultW <- getOption("warn")
options(warn = -1)

library(lubridate)
library(sparklyr)
library(tidyr)
library(dplyr)
library(stringr)

source("helper.R")
```



Extract transform and load data
```{R}

# Setup connection
sc <- ConnectToSpark()
sdf_sql(sc, "SET hive.exec.dynamic.partition.mode = 'nonstrict'")

mutex <- attachMutex("alma_pulse", 30)

tryCatch({
  # Constants
  DT_FORMAT = "%Y-%m-%dT%H:%M:%S"

  # Extract data
  date_start_query <- paste("SELECT max(ts) + interval 1 second AS value",
                            "FROM environment.alma_pulse")
  date_start <- (sdf_sql(sc, date_start_query) %>% collect())$value %>%
    format(DT_FORMAT)
  date_end <- today(tz="UTC") %>%
    format(DT_FORMAT)
  
  extract_command <- paste("chmod 755 extract.sh; ./extract.sh", date_start, date_end)
  system(extract_command)
  
  
  # import into environment.alma_pulse
  alma_pulse <- spark_read_csv(sc,
                 name="alma_pulse_csv", header = F, overwrite = T, null_value = "\r",
                 path="hdfs://casagzclem1/rawdata/environment/alma_pulse") %>%
    distinct() %>%    # to insure that previous failed execution don't cause repeating rows
    filter(is.na(V4)) %>%
    transmute(tag=V1, ts=V2, val=V3, batch=str_sub(V2, 1, 7)) %>%
    spark_write_table("environment.alma_pulse", mode="append")
  
  
  # Defragment the table if we are the last day of the month.
  today = date(now())
  last_day_of_month = date(ceiling_date(now(), "month") - period(1, "day"))
  if (today == last_day_of_month) {
    print("Defragmentation and cleanup of 'environment.alma_pulse'")
    sdf_sql(sc, "SELECT tag, ts, val, batch FROM environment.alma_pulse") %>%
      spark_write_table("environment.alma_pulse2", partition_by = "batch")
    invisible({
      sdf_sql(sc,"ALTER TABLE environment.alma_pulse RENAME TO environment.alma_pulse_tmp")
      sdf_sql(sc,"ALTER TABLE environment.alma_pulse_tmp RENAME TO environment.alma_pulse")
      sdf_sql(sc,"DROP TABLE environment.alma_pulse_tmp")
    })
  }
  
  # Cleanup
  system(paste('ssh casagzclem1 "impala-shell -q \'',
                 'INVALIDATE METADATA environment.alma_pulse;',
                 'REFRESH environment.alma_pulse;',
               '\'"'))
  
},
finally = {
  releaseMutex(mutex)
  spark_disconnect(sc)
  system("hdfs dfs -rm /rawdata/environment/alma_pulse/*")
})


```